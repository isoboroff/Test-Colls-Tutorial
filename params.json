{"name":"Test-colls-tutorial","tagline":"This site is for materials relating to a tutorial for SIGIR 2013 on building test collections.","body":"# Building Test Collections\r\n## An Interactive Tutorial for Students and Others Without Their Own Evaluation Conference Series\r\n## SIGIR 2013, Dublin, Ireland\r\n\r\nIan Soboroff\r\nNational Institute of Standards and Technology\r\n\r\nWhile existing test collections and evaluation conference efforts may sufficiently support one's research, one can easily find oneself wanting to solve problems no one else is solving yet.  But how can research in IR be done (or be published!) without solid data and experiments?  Not everyone can talk TREC, CLEF, INEX, or NTCIR into running a track to build a collection.  \r\n\r\nThis tutorial aims to teach how to build a test collection using resources at hand, how to measure the quality of that collection, how to understand its limitations, and how to communicate them.\r\nThe intended audience is advanced students who find themselves in need of a test collection, or actually in the process of building a test collection, to support their own research.   The goal of this tutorial is to lay out issues, procedures, pitfalls, and practical advice.\r\n\r\nAttendees should come with a specific current need for data, and/or details on their in-progress collection building effort.  The first half of the course will cover history, techniques, and research questions in a lecture format.  The second half will be entirely devoted to open discussion during which we will collaboratively work through problems the attendees are currently working on.\r\n\r\nUpon completion of this tutorial, attendees will be familiar with the history of the test collection evaluation paradigm; understand the process of beginning from a concrete user task and abstracting that to a test collection design; understand different ways of establishing a document collection; understand the process of topic development; understand how to operationalize the notion of relevance, and be familiar with issues surrounding elicitation of relevance judgments; understand the pooling methodologies for sampling documents for labeling, and be familiar with sampling strategies for reducing effort; be familiar with procedures for measuring and validating a test collection; and be familiar with current research issues in this area.\r\n\r\nThis tutorial is highly relevant to information retrieval researchers, especially students nearing the experimental phase of their research.  While numerous test collections have already been built and are ready for them to use, it is increasingly common that we wish to explore an information retrieval task for which no test collection yet exists.  The choice is to adapt an existing collection, design a new one, or move beyond the laboratory experiment paradigm.\r\n\r\nMaterials for the course will include lecture slides and a selection of papers drawn from the current literature for further reading.  These materials will be hosted here, and students with internet connectivity at the tutorial will be able to access them and follow along during the tutorial itself.\r\n\r\nTutorial topics may include:\r\n\r\n- Introduction to test collections: \r\nbasic concepts: task, documents, topics, relevance judgments, and measures;\r\nhistory of Cranfield paradigm.\r\n\r\n- Task: a task-centered approach to conceiving test collections;\r\nmetrics as an operationalization of task success;\r\nunderstanding the user task and the role of the system.\r\n\r\n- Documents:\r\nthe relationship between documents and task;\r\nnaturalism vs constructivism;\r\nopportunity sampling and bias;\r\ndistribution and sharing.\r\n\r\n- Topics:\r\ndesigning topics from a task perspective.\r\nsources for topics.\r\nexploration or topic development.\r\nextracting topics from logs.\r\ntopics and queries.\r\ntopic set size.\r\n\r\n- Relevance:\r\ndefining relevance and utility starting from the task;\r\nobtaining labels, explicit and implicit elicitation (highlighting);\r\nInterface considerations;\r\ninter-annotator agreement;\r\nerrors;\r\ncrowdsourcing for relevance judgments;\r\nvalidation, process control, quality assurance;\r\nannotator skill set.\r\n\r\n- Pooling:\r\nproblem of scale and bias;\r\nbreadth of pools, multiple systems;\r\ncompleteness vs. samples;\r\n\r\n- Validation:\r\nuser study;\r\nside-by-side comparison;\r\na/b testing;\r\ninterleaving.\r\n\r\n- Pooling and sampling:\r\npooling as a sampling method;\r\npooling as optimization;\r\nmove-to-front pooling;\r\nuniform sampling, stratified sampling, measure sampling;\r\nminimal test collections.\r\n\r\n- Advanced task concepts:\r\nfiltering, supporting system adaptation;\r\nsessions, time, user adaptation;\r\ncontext, feedback;\r\nexploration and fuzzy tasks;\r\nnovelty, differential relevance;\r\nfundamental limits of Cranfield.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}