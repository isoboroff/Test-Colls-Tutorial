
@book{voorhees_trec:_2005,
	title = {{TREC:} Experiment and Evaluation in Information Retrieval},
	isbn = {9780262220736},
	url = {http://mitpress.mit.edu/books/trec},
	abstract = {The Text {REtrieval} Conference ({TREC)}, a yearly workshop hosted by the {US} government's National Institute of Standards and Technology, provides the infrastructure necessary for large-scale evaluation of text retrieval methodologies. With the goal of accelerating research in this area, {TREC} created the first large test collections of full-text documents and standardized retrieval evaluation. The impact has been significant; since {TREC's} beginning in 1992, retrieval effectiveness has approximately doubled. {TREC} has built a variety of large test collections, including collections for such specialized retrieval tasks as cross-language retrieval and retrieval of speech. Moreover, {TREC} has accelerated the transfer of research ideas into commercial systems, as demonstrated in the number of retrieval techniques developed in {TREC} that are now used in Web search engines.

This book provides a comprehensive review of {TREC} research, summarizing the variety of {TREC} results, documenting the best practices in experimental information retrieval, and suggesting areas for further research. The first part of the book describes {TREC's} history, test collections, and retrieval methodology. Next, the book provides "track" reports—describing the evaluations of specific tasks, including routing and filtering, interactive retrieval, and retrieving noisy text. The final part of the book offers perspectives on {TREC} from such participants as Microsoft Research, University of Massachusetts, Cornell University, University of Waterloo, City University of New York, and {IBM.} The book will be of interest to researchers in information retrieval and related technologies, including natural language processing.},
	publisher = {{MIT} Press},
	editor = {Voorhees, Ellen M. and Harman, Donna K.},
	year = {2005}
},

@article{sanderson_test_2010,
	title = {Test Collection Based Evaluation of Information Retrieval Systems},
	volume = {4},
	issn = {1554-0669, 1554-0677},
	url = {http://www.nowpublishers.com/articles/foundations-and-trends-in-information-retrieval/INR-009},
	doi = {10.1561/1500000009},
	number = {4},
	urldate = {2013-07-11},
	journal = {Foundations and Trends® in Information Retrieval},
	author = {Sanderson, Mark},
	year = {2010},
	pages = {247--375},
	file = {now publishers – Test Collection Based Evaluation of Information Retrieval Systems:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/37XHIKSU/INR-009.html:text/html}
},

@techreport{sparck_jones_report_1975,
	title = {Report on the Need for and Provision for an {'IDEAL'} Information Retrieval Test Collection},
	url = {http://www.sigir.org/museum/allcontents.html},
	number = {5266},
	urldate = {2013-07-11},
	institution = {Computer Laboratory, University of Cambridge},
	author = {Sparck Jones, Karen and van Rijsbergen, C. J.},
	year = {1975},
	file = {ACM SIGIR - Museum Contents:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/NHWP53SZ/allcontents.html:text/html}
},

@techreport{gilbert_statistical_1979,
	title = {Statistical Bases of Relevance Assessment for the {'IDEAL'} Information Retrieval Test Collection},
	url = {http://www.sigir.org/museum/allcontents.html},
	number = {5481},
	urldate = {2013-07-11},
	institution = {Computer Laboratory, University of Cambridge},
	author = {Gilbert, H and Sparck Jones, Karen},
	year = {1979}
},

@techreport{sparck_jones_report_1977,
	title = {Report on a Design Study for the {'IDEAL'} Information Retrieval Test Collection},
	url = {http://www.sigir.org/museum/allcontents.html},
	number = {5428},
	urldate = {2013-07-11},
	institution = {Computer Laboratory, University of Cambridge},
	author = {Sparck Jones, Karen and Bates, R. G.},
	year = {1977}
},

@inproceedings{soboroff_dynamic_2006,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '06},
	title = {Dynamic test collections: measuring search effectiveness on the live web},
	isbn = {1-59593-369-7},
	shorttitle = {Dynamic test collections},
	url = {http://doi.acm.org/10.1145/1148170.1148220},
	doi = {10.1145/1148170.1148220},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian},
	year = {2006},
	keywords = {collections, retrieval, test},
	pages = {276–283},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/UX9CIRJH/Soboroff - 2006 - Dynamic test collections measuring search effecti.pdf:application/pdf}
},

@inproceedings{soboroff_building_2003,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '03},
	title = {Building a filtering test collection for {TREC} 2002},
	isbn = {1-58113-646-3},
	url = {http://doi.acm.org/10.1145/860435.860481},
	doi = {10.1145/860435.860481},
	abstract = {Test collections for the filtering track in {TREC} have typically used either past sets of relevance judgments, or categorized collections such as Reuters Corpus Volume 1 or {OHSUMED}, because filtering systems need relevance judgments during the experiment for training and adaptation. For {TREC} 2002, we constructed an entirely new set of search topics for the Reuters Corpus for measuring filtering systems. Our method for building the topics involved multiple iterations of feedback from assessors, and fusion of results from multiple search systems using different search algorithms. We also developed a second set of "inexpensive" topics based on categories in the document collection. We found that the initial judgments made for the experiment were sufficient; subsequent pooled judging changed system rankings very little. We also found that systems performed very differently on the category topics than on the assessor-built topics.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 26th annual international {ACM} {SIGIR} conference on Research and development in informaion retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian and Robertson, Stephen},
	year = {2003},
	pages = {243–250},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/KXAU8HQK/Soboroff and Robertson - 2003 - Building a filtering test collection for TREC 2002.pdf:application/pdf}
},

@inproceedings{soboroff_comparison_2007,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '07},
	title = {A comparison of pooled and sampled relevance judgments},
	isbn = {978-1-59593-597-7},
	url = {http://doi.acm.org/10.1145/1277741.1277908},
	doi = {10.1145/1277741.1277908},
	abstract = {Test collections are most useful when they are reusable, that is, when they can be reliably used to rank systems that did not contribute to the pools. Pooled relevance judgments for very large collections may not be reusable for two easons: they will be very sparse and not sufficiently complete, and they may be biased in the sense that theywill unfairly rank some class of systems. The {TREC} 2006 terabyte track judged both a pool and a deep random sample in order to measure the effects of sparseness and bias.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian},
	year = {2007},
	keywords = {pooling, sampling, test collections},
	pages = {785–786},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/UBXP8VHA/Soboroff - 2007 - A comparison of pooled and sampled relevance judgm.pdf:application/pdf}
},

@inproceedings{bailey_relevance_2008,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '08},
	title = {Relevance assessment: are judges exchangeable and does it matter},
	isbn = {978-1-60558-164-4},
	shorttitle = {Relevance assessment},
	url = {http://doi.acm.org/10.1145/1390334.1390447},
	doi = {10.1145/1390334.1390447},
	abstract = {We investigate to what extent people making relevance judgements for a reusable {IR} test collection are exchangeable. We consider three classes of judge: "gold standard" judges, who are topic originators and are experts in a particular information seeking task; "silver standard" judges, who are task experts but did not create topics; and "bronze standard" judges, who are those who did not define topics and are not experts in the task. Analysis shows low levels of agreement in relevance judgements between these three groups. We report on experiments to determine if this is sufficient to invalidate the use of a test collection for measuring system performance when relevance assessments have been created by silver standard or bronze standard judges. We find that both system scores and system rankings are subject to consistent but small differences across the three assessment sets. It appears that test collections are not completely robust to changes of judge when these judges vary widely in task and topic expertise. Bronze standard judges may not be able to substitute for topic and task experts, due to changes in the relative performance of assessed systems, and gold standard judges are preferred.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Bailey, Peter and Craswell, Nick and Soboroff, Ian and Thomas, Paul and de Vries, Arjen P. and Yilmaz, Emine},
	year = {2008},
	keywords = {inter-rater agreement, test collection relevance judgements},
	pages = {667–674},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/9PRRKFP4/Bailey et al. - 2008 - Relevance assessment are judges exchangeable and .pdf:application/pdf}
},

@inproceedings{buttcher_reliable_2007,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '07},
	title = {Reliable information retrieval evaluation with incomplete and biased judgements},
	isbn = {978-1-59593-597-7},
	url = {http://doi.acm.org/10.1145/1277741.1277755},
	doi = {10.1145/1277741.1277755},
	abstract = {Information retrieval evaluation based on the pooling method is inherently biased against systems that did not contribute to the pool of judged documents. This may distort the results obtained about the relative quality of the systems evaluated and thus lead to incorrect conclusions about the performance of a particular ranking technique. We examine the magnitude of this effect and explore how it can be countered by automatically building an unbiased set of judgements from the original, biased judgements obtained through pooling. We compare the performance of this method with other approaches to the problem of incomplete judgements, such as bpref, and show that the proposed method leads to higher evaluation accuracy, especially if the set of manual judgements is rich in documents, but highly biased against some systems.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 30th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Büttcher, Stefan and Clarke, Charles L. A. and Yeung, Peter C. K. and Soboroff, Ian},
	year = {2007},
	keywords = {evaluation, incomplete judgments, information retrieval},
	pages = {63–70},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/67TI5JM2/Büttcher et al. - 2007 - Reliable information retrieval evaluation with inc.pdf:application/pdf}
},

@inproceedings{soboroff_evaluating_2004,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '04},
	title = {On evaluating web search with very few relevant documents},
	isbn = {1-58113-881-4},
	url = {http://doi.acm.org/10.1145/1008992.1009105},
	doi = {10.1145/1008992.1009105},
	abstract = {Many common web searches by their nature have a very small number of relevant documents. Homepage and "namedpage" searching are known-item searches where there is only a single relevant document. Topic distillation is a special kind of topical relevance search where the user wishes to find a few key web sites rather than every relevant web page. Because these types of searches are so common, web search evaluations have come to focus on tasks where there are very few relevant documents. Evaluations with few relevant documents pose special challenges for current metrics. In particular, the {TREC} 2003 topic distillation evaluation is unable to distinguish most submitted runs from each other.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 27th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian},
	year = {2004},
	keywords = {measurement error, web search},
	pages = {530–531},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/BBC3I6ZQ/Soboroff - 2004 - On evaluating web search with very few relevant do.pdf:application/pdf}
},

@inproceedings{soboroff_ranking_2001,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '01},
	title = {Ranking retrieval systems without relevance judgments},
	isbn = {1-58113-331-6},
	url = {http://doi.acm.org/10.1145/383952.383961},
	doi = {10.1145/383952.383961},
	abstract = {The most prevalent experimental methodology for comparing the effectiveness of information retrieval systems requires a test collection, composed of a set of documents, a set of query topics, and a set of relevance judgments indicating which documents are relevant to which topics.  It is well known that relevance judgments are not infallible, but recent retrospective investigation into results from the Text {REtrieval} Conference ({TREC)} has shown that differences in human judgments of relevance do not affect the relative measured performance of retrieval systems.  Based on this result, we propose and describe the initial results of a new evaluation methodology which replaces human relevance judgments with a randomly selected mapping of documents to topics which we refer to aspseudo-relevance {judgments.Rankings} of systems with our methodology correlate positively with official {TREC} rankings, although the performance of the top systems is not predicted well.  The correlations are stable over a variety of pool depths and sampling techniques.  With improvements, such a methodology could be useful in evaluating systems such as World-Wide Web search engines, where the set of documents changes too often to make traditional collection construction techniques practical.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 24th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Soboroff, Ian and Nicholas, Charles and Cahan, Patrick},
	year = {2001},
	pages = {66–73},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/G4UVG2BV/Soboroff et al. - 2001 - Ranking retrieval systems without relevance judgme.pdf:application/pdf}
},

@article{buckley_bias_2007,
	title = {Bias and the limits of pooling for large collections},
	volume = {10},
	issn = {1386-4564},
	url = {http://dx.doi.org/10.1007/s10791-007-9032-x},
	doi = {10.1007/s10791-007-9032-x},
	abstract = {Modern retrieval test collections are built through a process called pooling in which only a sample of the entire document set is judged for each topic. The idea behind pooling is to find enough relevant documents such that when unjudged documents are assumed to be nonrelevant the resulting judgment set is sufficiently complete and unbiased. Yet a constant-size pool represents an increasingly small percentage of the document set as document sets grow larger, and at some point the assumption of approximately complete judgments must become invalid. This paper shows that the judgment sets produced by traditional pooling when the pools are too small relative to the total document set size can be biased in that they favor relevant documents that contain topic title words. This phenomenon is wholly dependent on the collection size and does not depend on the number of relevant documents for a given topic. We show that the {AQUAINT} test collection constructed in the recent {TREC} 2005 workshop exhibits this biased relevance set; it is likely that the test collections based on the much larger {GOV2} document set also exhibit the bias. The paper concludes with suggested modifications to traditional pooling and evaluation methodology that may allow very large reusable test collections to be built.},
	number = {6},
	urldate = {2013-07-11},
	journal = {Inf. Retr.},
	author = {Buckley, Chris and Dimmick, Darrin and Soboroff, Ian and Voorhees, Ellen},
	month = dec,
	year = {2007},
	keywords = {pooling, Sampling bias, test collections},
	pages = {491–508}
},

@inproceedings{buckley_evaluating_2000,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '00},
	title = {Evaluating evaluation measure stability},
	isbn = {1-58113-226-3},
	url = {http://doi.acm.org/10.1145/345508.345543},
	doi = {10.1145/345508.345543},
	abstract = {This paper presents a novel way of examining the accuracy of the evaluation measures commonly used in information retrieval experiments. It validates several of the rules-of-thumb experimenters use, such as the number of queries needed for a good experiment is at least 25 and 50 is better, while challenging other beliefs, such as the common evaluation measures are equally reliable. As an example, we show that Precision at 30 documents has about twice the average error rate as Average Precision has. These results can help information retrieval researchers design experiments that provide a desired level of confidence in their results. In particular, we suggest researchers using Web measures such as Precision at 10 documents will need to use many more than 50 queries or will have to require two methods to have a very large difference in evaluation scores before concluding that the two methods are actually different.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 23rd annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Buckley, Chris and Voorhees, Ellen M.},
	year = {2000},
	pages = {33–40},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/BPTJZI9U/Buckley and Voorhees - 2000 - Evaluating evaluation measure stability.pdf:application/pdf}
},

@inproceedings{voorhees_variations_1998,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '98},
	title = {Variations in relevance judgments and the measurement of retrieval effectiveness},
	isbn = {1-58113-015-5},
	url = {http://doi.acm.org/10.1145/290941.291017},
	doi = {10.1145/290941.291017},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Voorhees, Ellen M.},
	year = {1998},
	pages = {315–323},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/J54JMJMH/Voorhees - 1998 - Variations in relevance judgments and the measurem.pdf:application/pdf}
},

@inproceedings{voorhees_philosophy_2002,
	address = {London, {UK}, {UK}},
	series = {{CLEF} '01},
	title = {The Philosophy of Information Retrieval Evaluation},
	isbn = {3-540-44042-9},
	url = {http://dl.acm.org/citation.cfm?id=648264.753539},
	urldate = {2013-07-11},
	booktitle = {Revised Papers from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems},
	publisher = {Springer-Verlag},
	author = {Voorhees, Ellen M.},
	year = {2002},
	pages = {355–370}
},

@inproceedings{bernstein_redundant_2005,
	address = {New York, {NY}, {USA}},
	series = {{CIKM} '05},
	title = {Redundant documents and search effectiveness},
	isbn = {1-59593-140-6},
	url = {http://doi.acm.org/10.1145/1099554.1099733},
	doi = {10.1145/1099554.1099733},
	abstract = {The web contains a great many documents that are content-equivalent, that is, informationally redundant with respect to each other. The presence of such mutually redundant documents in search results can degrade the user search experience. Previous attempts to address this issue, most notably the {TREC} novelty track, were characterized by difficulties with accuracy and evaluation. In this paper we explore syntactic techniques --- particularly document fingerprinting --- for detecting content equivalence. Using these techniques on the {TREC} {GOV1} and {GOV2} corpora revealed a high degree of redundancy; a user study confirmed that our metrics were accurately identifying content-equivalence. We show, moreover, that content-equivalent documents have a significant effect on the search experience: we found that 16.6\% of all relevant documents in runs submitted to the {TREC} 2004 terabyte track were redundant.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 14th {ACM} international conference on Information and knowledge management},
	publisher = {{ACM}},
	author = {Bernstein, Yaniv and Zobel, Justin},
	year = {2005},
	keywords = {duplicate detection, novelty, search effectiveness},
	pages = {736–743},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/6PBBFK3P/Bernstein and Zobel - 2005 - Redundant documents and search effectiveness.pdf:application/pdf}
},

@inproceedings{zobel_how_1998,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '98},
	title = {How reliable are the results of large-scale information retrieval experiments?},
	isbn = {1-58113-015-5},
	url = {http://doi.acm.org/10.1145/290941.291014},
	doi = {10.1145/290941.291014},
	abstract = {Two stages in measurement of techniques for information
retrieval are gathering of documents for relevance assessment and
use of the assessments to numerically evaluate effectiveness. We
consider both of these stages in the context of the {TREC}
experiments, to determine whether they lead to measurements that
are trustworthy and fair. Our detailed empirical investigation of
the {TREC} results shows that the measured relative performance of
systems appears to be reliable, but that recall is overestimated:
it is likely that many relevant documents have not been found. We
propose a new pooling strategy that can significantly in- crease
the number of relevant documents found for given effort, without
compromising fairness.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Zobel, Justin},
	year = {1998},
	pages = {307–314},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/VHT3WWDQ/Zobel - 1998 - How reliable are the results of large-scale inform.pdf:application/pdf}
},

@inproceedings{yilmaz_simple_2008,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '08},
	title = {A simple and efficient sampling method for estimating {AP} and {NDCG}},
	isbn = {978-1-60558-164-4},
	url = {http://doi.acm.org/10.1145/1390334.1390437},
	doi = {10.1145/1390334.1390437},
	abstract = {We consider the problem of large scale retrieval evaluation. Recently two methods based on random sampling were proposed as a solution to the extensive effort required to judge tens of thousands of documents. While the first method proposed by Aslam et al. [1] is quite accurate and efficient, it is overly complex, making it difficult to be used by the community, and while the second method proposed by Yilmaz et al., {infAP} [14], is relatively simple, it is less efficient than the former since it employs uniform random sampling from the set of complete judgments. Further, none of these methods provide confidence intervals on the estimated values. The contribution of this paper is threefold: (1) we derive confidence intervals for {infAP}, (2) we extend {infAP} to incorporate nonrandom relevance judgments by employing stratified random sampling, hence combining the efficiency of stratification with the simplicity of random sampling, (3) we describe how this approach can be utilized to estimate {nDCG} from incomplete judgments. We validate the proposed methods using {TREC} data and demonstrate that these new methods can be used to incorporate nonrandom samples, as were available in {TREC} Terabyte track '06.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Yilmaz, Emine and Kanoulas, Evangelos and Aslam, Javed A.},
	year = {2008},
	keywords = {average precision, evaluation, incomplete judgments, {infAP}, {nDCG}, sampling},
	pages = {603–610},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/2ZG4CINM/Yilmaz et al. - 2008 - A simple and efficient sampling method for estimat.pdf:application/pdf}
},

@inproceedings{carterette_evaluation_2008,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '08},
	title = {Evaluation over thousands of queries},
	isbn = {978-1-60558-164-4},
	url = {http://doi.acm.org/10.1145/1390334.1390445},
	doi = {10.1145/1390334.1390445},
	abstract = {Information retrieval evaluation has typically been performed over several dozen queries, each judged to near-completeness. There has been a great deal of recent work on evaluation over much smaller judgment sets: how to select the best set of documents to judge and how to estimate evaluation measures when few judgments are available. In light of this, it should be possible to evaluate over many more queries without much more total judging effort. The Million Query Track at {TREC} 2007 used two document selection algorithms to acquire relevance judgments for more than 1,800 queries. We present results of the track, along with deeper analysis: investigating tradeoffs between the number of queries and number of judgments shows that, up to a point, evaluation over more queries with fewer judgments is more cost-effective and as reliable as fewer queries with more judgments. Total assessor effort can be reduced by 95\% with no appreciable increase in evaluation errors.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Carterette, Ben and Pavlu, Virgil and Kanoulas, Evangelos and Aslam, Javed A. and Allan, James},
	year = {2008},
	keywords = {evaluation, information retrieval, million query track, test collections},
	pages = {651–658},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/H4STSM7C/Carterette et al. - 2008 - Evaluation over thousands of queries.pdf:application/pdf}
},

@inproceedings{aslam_unified_2003,
	address = {New York, {NY}, {USA}},
	series = {{CIKM} '03},
	title = {A unified model for metasearch, pooling, and system evaluation},
	isbn = {1-58113-723-0},
	url = {http://doi.acm.org/10.1145/956863.956953},
	doi = {10.1145/956863.956953},
	abstract = {We present a unified model which, given the ranked lists of documents returned by multiple retrieval systems in response to a given query, simultaneously solves the problems of (1) fusing the ranked lists of documents in order to obtain a high-quality combined list (metasearch); (2) generating document collections likely to contain large fractions of relevant documents (pooling); and (3) accurately evaluating the underlying retrieval systems with small numbers of relevance judgments (efficient system assessment). Our approach is based on the Hedge algorithm for on-line learning. In effect, our proposed system "learns" which documents are likely to be relevant from a sequence of on-line relevance judgments. In experiments using {TREC} data, our methodology is shown to outperform standard methods for metasearch, pooling, and system evaluation, often remarkably so.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the twelfth international conference on Information and knowledge management},
	publisher = {{ACM}},
	author = {Aslam, Javed A. and Pavlu, Virgiliu and Savell, Robert},
	year = {2003},
	keywords = {active learning, evaluation, metasearch, pooling},
	pages = {484–491},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/ZHACPTVH/Aslam et al. - 2003 - A unified model for metasearch, pooling, and syste.pdf:application/pdf}
},

@inproceedings{aslam_effectiveness_2003,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '03},
	title = {On the effectiveness of evaluating retrieval systems in the absence of relevance judgments},
	isbn = {1-58113-646-3},
	url = {http://doi.acm.org/10.1145/860435.860501},
	doi = {10.1145/860435.860501},
	abstract = {Soboroff, Nicholas and Cahan recently proposed a method for evaluating the performance of retrieval systems without relevance judgments. They demonstrated that the system evaluations produced by their methodology are correlated with actual evaluations using relevance judgments in the {TREC} competition. In this work, we propose an explanation for this phenomenon. We devise a simple measure for quantifying the similarity of retrieval systems by assessing the similarity of their retrieved results. Then, given a collection of retrieval systems and their retrieved results, we use this measure to assess the average similarity of a system to the other systems in the collection. We demonstrate that evaluating retrieval systems according to average similarity yields results quite similar to the methodology proposed by Soboroff et{\textasciitilde}al., and we further demonstrate that these two techniques are in fact highly correlated. Thus, the techniques are effectively evaluating and ranking retrieval systems by "popularity" as opposed to "performance.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 26th annual international {ACM} {SIGIR} conference on Research and development in informaion retrieval},
	publisher = {{ACM}},
	author = {Aslam, Javed A. and Savell, Robert},
	year = {2003},
	keywords = {ranking, retrieval, systems},
	pages = {361–362},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/D6P7U8T4/Aslam and Savell - 2003 - On the effectiveness of evaluating retrieval syste.pdf:application/pdf}
},

@inproceedings{yilmaz_new_2008,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '08},
	title = {A new rank correlation coefficient for information retrieval},
	isbn = {978-1-60558-164-4},
	url = {http://doi.acm.org/10.1145/1390334.1390435},
	doi = {10.1145/1390334.1390435},
	abstract = {In the field of information retrieval, one is often faced with the problem of computing the correlation between two ranked lists. The most commonly used statistic that quantifies this correlation is Kendall's Τ. Often times, in the information retrieval community, discrepancies among those items having high rankings are more important than those among items having low rankings. The Kendall's Τ statistic, however, does not make such distinctions and equally penalizes errors both at high and low rankings. In this paper, we propose a new rank correlation coefficient, {AP} correlation (Τap), that is based on average precision and has a probabilistic interpretation. We show that the proposed statistic gives more weight to the errors at high rankings and has nice mathematical properties which make it easy to interpret. We further validate the applicability of the statistic using experimental data.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 31st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Yilmaz, Emine and Aslam, Javed A. and Robertson, Stephen},
	year = {2008},
	keywords = {average precision, evaluation, Kendall's tau, rank correlation},
	pages = {587–594},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/DWG8GVU7/Yilmaz et al. - 2008 - A new rank correlation coefficient for information.pdf:application/pdf}
},

@inproceedings{aslam_statistical_2006,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '06},
	title = {A statistical method for system evaluation using incomplete judgments},
	isbn = {1-59593-369-7},
	url = {http://doi.acm.org/10.1145/1148170.1148263},
	doi = {10.1145/1148170.1148263},
	abstract = {We consider the problem of large-scale retrieval evaluation, and we propose a statistical method for evaluating retrieval systems using incomplete judgments. Unlike existing techniques that (1) rely on effectively complete, and thus prohibitively expensive, relevance judgment sets, (2) produce biased estimates of standard performance measures, or (3) produce estimates of non-standard measures thought to be correlated with these standard measures, our proposed statistical technique produces unbiased estimates of the standard measures {themselves.Our} proposed technique is based on random sampling. While our estimates are unbiased by statistical design, their variance is dependent on the sampling distribution employed; as such, we derive a sampling distribution likely to yield low variance estimates. We test our proposed technique using benchmark {TREC} data, demonstrating that a sampling pool derived from a set of runs can be used to efficiently and effectively evaluate those runs. We further show that these sampling pools generalize well to unseen runs. Our experiments indicate that highly accurate estimates of standard performance measures can be obtained using a number of relevance judgments as small as 4\% of the typical {TREC-style} judgment pool.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Aslam, Javed A. and Pavlu, Virgil and Yilmaz, Emine},
	year = {2006},
	keywords = {average precision, evaluation, sampling},
	pages = {541–548},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/RN8ZIGGM/Aslam et al. - 2006 - A statistical method for system evaluation using i.pdf:application/pdf}
},

@inproceedings{aslam_inferring_2006,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '06},
	title = {Inferring document relevance via average precision},
	isbn = {1-59593-369-7},
	url = {http://doi.acm.org/10.1145/1148170.1148275},
	doi = {10.1145/1148170.1148275},
	abstract = {We consider the problem of evaluating retrieval systems using a limited number of relevance judgments. Recent work has demonstrated that one can accurately estimate average precision via a judged pool corresponding to a relatively small random sample of documents. In this work, we demonstrate that given values or estimates of average precision, one can accurately infer the relevances of unjudged documents. Combined, we thus show how one can efficiently and accurately infer a large judged pool from a relatively small number of judged documents, thus permitting accurate and efficient retrieval evaluation on a large scale.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 29th annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Aslam, Javed A. and Yilmaz, Emine},
	year = {2006},
	keywords = {average precision, relevance judgments},
	pages = {601–602},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/E8U9P6NQ/Aslam and Yilmaz - 2006 - Inferring document relevance via average precision.pdf:application/pdf}
},

@inproceedings{ipeirotis_managing_2011,
	address = {New York, {NY}, {USA}},
	series = {{WWW} '11},
	title = {Managing crowdsourced human computation: a tutorial},
	isbn = {978-1-4503-0637-9},
	shorttitle = {Managing crowdsourced human computation},
	url = {http://doi.acm.org/10.1145/1963192.1963314},
	doi = {10.1145/1963192.1963314},
	abstract = {The tutorial covers an emerging topic of wide interest: Crowdsourcing. Specifically, we cover areas of crowdsourcing related to managing structured and unstructured data in a web-related content. Many researchers and practitioners today see the great opportunity that becomes available through easily-available crowdsourcing platforms. However, most newcomers face the same questions: How can we manage the (noisy) crowds to generate high quality output? How to estimate the quality of the contributors? How can we best structure the tasks? How can we get results in small amounts of time and minimizing the necessary resources? How to setup the incentives? How should such crowdsourcing markets be setup? Their presented material will cover topics from a variety of fields, including computer science, statistics, economics, and psychology. Furthermore, the material will include real-life examples and case studies from years of experience in running and managing crowdsourcing applications in business settings.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 20th international conference companion on World wide web},
	publisher = {{ACM}},
	author = {Ipeirotis, Panagiotis G. and Paritosh, Praveen K.},
	year = {2011},
	keywords = {crowdsourcing, human computation, incentives, market design, mechanical turk, quality assurance, reputation, workflow control},
	pages = {287–288},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/I38R6XT5/Ipeirotis and Paritosh - 2011 - Managing crowdsourced human computation a tutoria.pdf:application/pdf}
},

@inproceedings{ipeirotis_quality_2010,
	address = {New York, {NY}, {USA}},
	series = {{HCOMP} '10},
	title = {Quality management on Amazon Mechanical Turk},
	isbn = {978-1-4503-0222-7},
	url = {http://doi.acm.org/10.1145/1837885.1837906},
	doi = {10.1145/1837885.1837906},
	abstract = {Crowdsourcing services, such as Amazon Mechanical Turk, allow for easy distribution of small tasks to a large number of workers. Unfortunately, since manually verifying the quality of the submitted results is hard, malicious workers often take advantage of the verification difficulty and submit answers of low quality. Currently, most requesters rely on redundancy to identify the correct answers. However, redundancy is not a panacea. Massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. Therefore, we need techniques that will accurately estimate the quality of the workers, allowing for the rejection and blocking of the low-performing workers and spammers. However, existing techniques cannot separate the true (unrecoverable) error rate from the (recoverable) biases that some workers exhibit. This lack of separation leads to incorrect assessments of a worker's quality. We present algorithms that improve the existing state-of-the-art techniques, enabling the separation of bias and error. Our algorithm generates a scalar score representing the inherent quality of each worker. We illustrate how to incorporate cost-sensitive classification errors in the overall framework and how to seamlessly integrate unsupervised and supervised techniques for inferring the quality of the workers. We present experimental results demonstrating the performance of the proposed algorithm under a variety of settings.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the {ACM} {SIGKDD} Workshop on Human Computation},
	publisher = {{ACM}},
	author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
	year = {2010},
	pages = {64–67},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/F96G2C3R/Ipeirotis et al. - 2010 - Quality management on Amazon Mechanical Turk.pdf:application/pdf}
},

@inproceedings{sheng_get_2008,
	address = {New York, {NY}, {USA}},
	series = {{KDD} '08},
	title = {Get another label? improving data quality and data mining using multiple, noisy labelers},
	isbn = {978-1-60558-193-4},
	shorttitle = {Get another label?},
	url = {http://doi.acm.org/10.1145/1401890.1401965},
	doi = {10.1145/1401890.1401965},
	abstract = {This paper addresses the repeated acquisition of labels for data items when the labeling is imperfect. We examine the improvement (or lack thereof) in data quality via repeated labeling, and focus especially on the improvement of training labels for supervised induction. With the outsourcing of small tasks becoming easier, for example via Rent-A-Coder or Amazon's Mechanical Turk, it often is possible to obtain less-than-expert labeling at low cost. With low-cost labeling, preparing the unlabeled part of the data can become considerably more expensive than labeling. We present repeated-labeling strategies of increasing complexity, and show several main results. (i) Repeated-labeling can improve label quality and model quality, but not always. (ii) When labels are noisy, repeated labeling can be preferable to single labeling even in the traditional setting where labels are not particularly cheap. (iii) As soon as the cost of processing the unlabeled data is not free, even the simple strategy of labeling everything multiple times can give considerable advantage. (iv) Repeatedly labeling a carefully chosen set of points is generally preferable, and we present a robust technique that combines different notions of uncertainty to select data points for which quality should be improved. The bottom line: the results show clearly that when labeling is not perfect, selective acquisition of multiple labels is a strategy that data miners should have in their repertoire; for certain label-quality/cost regimes, the benefit is substantial.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 14th {ACM} {SIGKDD} international conference on Knowledge discovery and data mining},
	publisher = {{ACM}},
	author = {Sheng, Victor S. and Provost, Foster and Ipeirotis, Panagiotis G.},
	year = {2008},
	keywords = {data preprocessing, data selection},
	pages = {614–622},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/HBG7ZCGP/Sheng et al. - 2008 - Get another label improving data quality and data.pdf:application/pdf}
},

@misc{panos_ipeirotis_managing_2011,
	title = {Managing Crowdsourced Human Computation: A Tutorial},
	shorttitle = {Managing Crowdsourced Human Computation},
	url = {http://www.slideshare.net/ipeirotis/managing-crowdsourced-human-computation},
	abstract = {The slides from the tutorial presented during the {WWW2011} conference in Hyderabad, India (March 29th 2011, by Panos Ipeirotis)},
	urldate = {2013-07-11},
	author = {Panos Ipeirotis},
	month = aug,
	year = {2011}
},

@inproceedings{kazai_analysis_2012,
	address = {New York, {NY}, {USA}},
	series = {{CIKM} '12},
	title = {An analysis of systematic judging errors in information retrieval},
	isbn = {978-1-4503-1156-4},
	url = {http://doi.acm.org/10.1145/2396761.2396779},
	doi = {10.1145/2396761.2396779},
	abstract = {Test collections are powerful mechanisms for the evaluation and optimization of information retrieval systems. However, there is reported evidence that experiment outcomes can be affected by changes to the judging guidelines or changes in the judge population. This paper examines such effects in a web search setting, comparing the judgments of four groups of judges: {NIST} Web Track judges, untrained crowd workers and two groups of trained judges of a commercial search engine. Our goal is to identify systematic judging errors by comparing the labels contributed by the different groups, working under the same or different judging guidelines. In particular, we focus on detecting systematic differences in judging depending on specific characteristics of the queries and {URLs.} For example, we ask whether a given population of judges, working under a given set of judging guidelines, are more likely to consistently overrate Wikipedia pages than another group judging under the same instructions. Our approach is to identify judging errors with respect to a consensus set, a judged gold set and a set of user clicks. We further demonstrate how such biases can affect the training of retrieval systems.},
	urldate = {2013-07-11},
	booktitle = {Proceedings of the 21st {ACM} international conference on Information and knowledge management},
	publisher = {{ACM}},
	author = {Kazai, Gabriella and Craswell, Nick and Yilmaz, Emine and Tahaghoghi, {S.M.M}},
	year = {2012},
	keywords = {bias, noise, relevence},
	pages = {105–114},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/WEFCC3FV/Kazai et al. - 2012 - An analysis of systematic judging errors in inform.pdf:application/pdf}
},

@article{alonso_implementing_2013,
	title = {Implementing crowdsourcing-based relevance experimentation: an industrial perspective},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	shorttitle = {Implementing crowdsourcing-based relevance experimentation},
	url = {http://link.springer.com/article/10.1007/s10791-012-9204-1},
	doi = {10.1007/s10791-012-9204-1},
	abstract = {Crowdsourcing has emerged as a viable platform for conducting different types of relevance evaluation. The main reason behind this trend is that it makes possible to conduct experiments extremely fast, with good results at a low cost. However, like in any experiment, there are several implementation details that would make an experiment work or fail. To gather useful results, clear instructions, user interface guidelines, content quality, inter-rater agreement metrics, work quality, and worker feedback are important characteristics of a successful crowdsourcing experiment. Furthermore, designing and implementing experiments that require thousands or millions of labels is different than conducting small scale research investigations. In this paper we outline a framework for conducting continuous crowdsourcing experiments, emphasizing aspects that should be of importance for all sorts of tasks. We illustrate the value of characteristics that can impact the overall outcome using examples based on {TREC}, {INEX}, and Wikipedia data sets.},
	language = {en},
	number = {2},
	urldate = {2013-07-11},
	journal = {Information Retrieval},
	author = {Alonso, Omar},
	month = apr,
	year = {2013},
	keywords = {crowdsourcing, Data Mining and Knowledge Discovery, Data Structures, Cryptology and Information Theory, Document Preparation and Text Processing, Experiment design, Information Storage and Retrieval, Methodology, Pattern Recognition, Relevance assessment \& evaluation},
	pages = {101--120},
	file = {Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/2MIJ5R7X/Alonso - 2013 - Implementing crowdsourcing-based relevance experim.pdf:application/pdf;Snapshot:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/IV9Z9HKD/s10791-012-9204-1.html:text/html}
},

@article{kazai_analysis_2013,
	title = {An analysis of human factors and label accuracy in crowdsourcing relevance judgments},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/article/10.1007/s10791-012-9205-0},
	doi = {10.1007/s10791-012-9205-0},
	abstract = {Crowdsourcing relevance judgments for the evaluation of search engines is used increasingly to overcome the issue of scalability that hinders traditional approaches relying on a fixed group of trusted expert judges. However, the benefits of crowdsourcing come with risks due to the engagement of a self-forming group of individuals—the crowd, motivated by different incentives, who complete the tasks with varying levels of attention and success. This increases the need for a careful design of crowdsourcing tasks that attracts the right crowd for the given task and promotes quality work. In this paper, we describe a series of experiments using Amazon’s Mechanical Turk, conducted to explore the ‘human’ characteristics of the crowds involved in a relevance assessment task. In the experiments, we vary the level of pay offered, the effort required to complete a task and the qualifications required of the workers. We observe the effects of these variables on the quality of the resulting relevance labels, measured based on agreement with a gold set, and correlate them with self-reported measures of various human factors. We elicit information from the workers about their motivations, interest and familiarity with the topic, perceived task difficulty, and satisfaction with the offered pay. We investigate how these factors combine with aspects of the task design and how they affect the accuracy of the resulting relevance labels. Based on the analysis of 960 {HITs} and 2,880 {HIT} assignments resulting in 19,200 relevance labels, we arrive at insights into the complex interaction of the observed factors and provide practical guidelines to crowdsourcing practitioners. In addition, we highlight challenges in the data analysis that stem from the peculiarity of the crowdsourcing environment where the sample of individuals engaged in specific work conditions are inherently influenced by the conditions themselves.},
	language = {en},
	number = {2},
	urldate = {2013-07-11},
	journal = {Information Retrieval},
	author = {Kazai, Gabriella and Kamps, Jaap and Milic-Frayling, Natasa},
	month = apr,
	year = {2013},
	keywords = {crowdsourcing, Data Mining and Knowledge Discovery, Data Structures, Cryptology and Information Theory, Document Preparation and Text Processing, Information Storage and Retrieval, Pattern Recognition, relevance judgments, Study of human factors},
	pages = {138--178},
	file = {Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/H85Z8XAR/Kazai et al. - 2013 - An analysis of human factors and label accuracy in.pdf:application/pdf;Snapshot:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/88HRZ3HK/s10791-012-9205-0.html:text/html}
},

@article{eickhoff_increasing_2013,
	title = {Increasing cheat robustness of crowdsourcing tasks},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	url = {http://link.springer.com/article/10.1007/s10791-011-9181-9},
	doi = {10.1007/s10791-011-9181-9},
	abstract = {Crowdsourcing successfully strives to become a widely used means of collecting large-scale scientific corpora. Many research fields, including Information Retrieval, rely on this novel way of data acquisition. However, it seems to be undermined by a significant share of workers that are primarily interested in producing quick generic answers rather than correct ones in order to optimise their time-efficiency and, in turn, earn more money. Recently, we have seen numerous sophisticated schemes of identifying such workers. Those, however, often require additional resources or introduce artificial limitations to the task. In this work, we take a different approach by investigating means of a priori making crowdsourced tasks more resistant against cheaters.},
	language = {en},
	number = {2},
	urldate = {2013-07-11},
	journal = {Information Retrieval},
	author = {Eickhoff, Carsten and Vries, Arjen P. de},
	month = apr,
	year = {2013},
	keywords = {crowdsourcing, Data Mining and Knowledge Discovery, Data Structures, Cryptology and Information Theory, Document Preparation and Text Processing, Human factors, Information Storage and Retrieval, Pattern Recognition, Stability, User experiments},
	pages = {121--137},
	file = {Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/3AHSKNB3/Eickhoff and Vries - 2013 - Increasing cheat robustness of crowdsourcing tasks.pdf:application/pdf;Snapshot:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/XJBH4BA2/s10791-011-9181-9.html:text/html}
},

@article{zuccon_crowdsourcing_2013,
	title = {Crowdsourcing interactions: using crowdsourcing for evaluating interactive information retrieval systems},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	shorttitle = {Crowdsourcing interactions},
	url = {http://link.springer.com/article/10.1007/s10791-012-9206-z},
	doi = {10.1007/s10791-012-9206-z},
	abstract = {In the field of information retrieval ({IR)}, researchers and practitioners are often faced with a demand for valid approaches to evaluate the performance of retrieval systems. The Cranfield experiment paradigm has been dominant for the in-vitro evaluation of {IR} systems. Alternative to this paradigm, laboratory-based user studies have been widely used to evaluate interactive information retrieval ({IIR)} systems, and at the same time investigate users’ information searching behaviours. Major drawbacks of laboratory-based user studies for evaluating {IIR} systems include the high monetary and temporal costs involved in setting up and running those experiments, the lack of heterogeneity amongst the user population and the limited scale of the experiments, which usually involve a relatively restricted set of users. In this paper, we propose an alternative experimental methodology to laboratory-based user studies. Our novel experimental methodology uses a crowdsourcing platform as a means of engaging study participants. Through crowdsourcing, our experimental methodology can capture user interactions and searching behaviours at a lower cost, with more data, and within a shorter period than traditional laboratory-based user studies, and therefore can be used to assess the performances of {IIR} systems. In this article, we show the characteristic differences of our approach with respect to traditional {IIR} experimental and evaluation procedures. We also perform a use case study comparing crowdsourcing-based evaluation with laboratory-based evaluation of {IIR} systems, which can serve as a tutorial for setting up crowdsourcing-based {IIR} evaluations.},
	language = {en},
	number = {2},
	urldate = {2013-07-11},
	journal = {Information Retrieval},
	author = {Zuccon, Guido and Leelanupab, Teerapong and Whiting, Stewart and Yilmaz, Emine and Jose, Joemon M. and Azzopardi, Leif},
	month = apr,
	year = {2013},
	keywords = {Crowdsourcing evaluation, Data Mining and Knowledge Discovery, Data Structures, Cryptology and Information Theory, Document Preparation and Text Processing, Information Storage and Retrieval, Interactions, Interactive {IR} evaluation, Pattern Recognition},
	pages = {267--305},
	file = {Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/U699N76E/Zuccon et al. - 2013 - Crowdsourcing interactions using crowdsourcing fo.pdf:application/pdf;Snapshot:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/IJITFWSH/s10791-012-9206-z.html:text/html}
},

@article{lease_crowdsourcing_2013,
	title = {Crowdsourcing for information retrieval: introduction to the special issue},
	volume = {16},
	issn = {1386-4564, 1573-7659},
	shorttitle = {Crowdsourcing for information retrieval},
	url = {http://link.springer.com/article/10.1007/s10791-013-9222-7},
	doi = {10.1007/s10791-013-9222-7},
	abstract = {This introduction to the special issue summarizes and contextualizes six novel research contributions at the intersection of information retrieval ({IR)} and crowdsourcing (also overlapping crowdsourcing’s closely-related sibling, human computation). Several of the papers included in this special issue represent deeper investigations into research topics for which earlier stages of the authors’ research were disseminated at crowdsourcing workshops at {SIGIR} and {WSDM} conferences, as well as at the {NIST} {TREC} conference. Since the first proposed use of crowdsourcing for {IR} in 2008, interest in this area has quickly accelerated and led to three workshops, an ongoing {NIST} {TREC} track, and a great variety of published papers, talks, and tutorials. We briefly summarize the area in order to help situate the contributions appearing in this special issue. We also discuss some broader current trends and issues in crowdsourcing which bear upon its use in {IR} and other fields.},
	language = {en},
	number = {2},
	urldate = {2013-07-11},
	journal = {Information Retrieval},
	author = {Lease, Matthew and Yilmaz, Emine},
	month = apr,
	year = {2013},
	keywords = {crowdsourcing, Data Mining and Knowledge Discovery, Data Structures, Cryptology and Information Theory, Document Preparation and Text Processing, human computation, Information Storage and Retrieval, Pattern Recognition, Search evaluation},
	pages = {91--100},
	file = {Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/VUENS8PU/Lease and Yilmaz - 2013 - Crowdsourcing for information retrieval introduct.pdf:application/pdf;Snapshot:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/J9EPK5J3/s10791-013-9222-7.html:text/html}
},

@inproceedings{cormack_efficient_1998,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '98},
	title = {Efficient construction of large test collections},
	isbn = {1-58113-015-5},
	url = {http://doi.acm.org/10.1145/290941.291009},
	doi = {10.1145/290941.291009},
	urldate = {2013-07-14},
	booktitle = {Proceedings of the 21st annual international {ACM} {SIGIR} conference on Research and development in information retrieval},
	publisher = {{ACM}},
	author = {Cormack, Gordon V. and Palmer, Christopher R. and Clarke, Charles L. A.},
	year = {1998},
	pages = {282–289},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/EPJ5FABX/Cormack et al. - 1998 - Efficient construction of large test collections.pdf:application/pdf}
},

@techreport{macdonald_trec_2006,
	title = {The {TREC} Blogs06 Collection : Creating and Analysing a Blog Test Collection},
	url = {http://ir.dcs.gla.ac.uk/terrier/publications/macdonald06creating.pdf},
	number = {{TR-2006-224}},
	institution = {Department of Computing Science, University of Glasgow},
	author = {Macdonald, C. and Ounis, I.},
	year = {2006},
	file = {macdonald06creating.pdf:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/PI7QQV9E/macdonald06creating.pdf:application/pdf}
},

@article{piwowarski_sound_2008,
	title = {Sound and complete relevance assessment for {XML} retrieval},
	volume = {27},
	issn = {1046-8188},
	url = {http://doi.acm.org/10.1145/1416950.1416951},
	doi = {10.1145/1416950.1416951},
	abstract = {In information retrieval research, comparing retrieval approaches requires test collections consisting of documents, user requests and relevance assessments. Obtaining relevance assessments that are as sound and complete as possible is crucial for the comparison of retrieval approaches. In {XML} retrieval, the problem of obtaining sound and complete relevance assessments is further complicated by the structural relationships between retrieval results. A major difference between {XML} retrieval and flat document retrieval is that the relevance of elements (the retrievable units) is not independent of that of related elements. This has major consequences for the gathering of relevance assessments. This article describes investigations into the creation of sound and complete relevance assessments for the evaluation of content-oriented {XML} retrieval as carried out at {INEX}, the evaluation campaign for {XML} retrieval. The campaign, now in its seventh year, has had three substantially different approaches to gather assessments and has finally settled on a highlighting method for marking relevant passages within documents—even though the objective is to collect assessments at element level. The different methods of gathering assessments at {INEX} are discussed and contrasted. The highlighting method is shown to be the most reliable of the methods.},
	number = {1},
	urldate = {2013-07-19},
	journal = {{ACM} Trans. Inf. Syst.},
	author = {Piwowarski, Benjamin and Trotman, Andrew and Lalmas, Mounia},
	month = dec,
	year = {2008},
	keywords = {evaluation, {INEX}, passage retrieval, Relevance assessment, {XML}, {XML} retrieval},
	pages = {1:1–1:37},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/HMMZEQ5E/Piwowarski et al. - 2008 - Sound and complete relevance assessment for XML re.pdf:application/pdf}
},

@inproceedings{asadi_pseudo_2011,
	address = {New York, {NY}, {USA}},
	series = {{SIGIR} '11},
	title = {Pseudo test collections for learning web search ranking functions},
	isbn = {978-1-4503-0757-4},
	url = {http://doi.acm.org/10.1145/2009916.2010058},
	doi = {10.1145/2009916.2010058},
	abstract = {Test collections are the primary drivers of progress in information retrieval. They provide yardsticks for assessing the effectiveness of ranking functions in an automatic, rapid, and repeatable fashion and serve as training data for learning to rank models. However, manual construction of test collections tends to be slow, labor-intensive, and expensive. This paper examines the feasibility of constructing web search test collections in a completely unsupervised manner given only a large web corpus as input. Within our proposed framework, anchor text extracted from the web graph is treated as a pseudo query log from which pseudo queries are sampled. For each pseudo query, a set of relevant and non-relevant documents are selected using a variety of web-specific features, including spam and aggregated anchor text weights. The automatically mined queries and judgments form a pseudo test collection that can be used for training ranking functions. Experiments carried out on {TREC} web track data show that learning to rank models trained using pseudo test collections outperform an unsupervised ranking function and are statistically indistinguishable from a model trained using manual judgments, demonstrating the usefulness of our approach in extracting reasonable quality training data "for free".},
	urldate = {2013-07-19},
	booktitle = {Proceedings of the 34th international {ACM} {SIGIR} conference on Research and development in Information Retrieval},
	publisher = {{ACM}},
	author = {Asadi, Nima and Metzler, Donald and Elsayed, Tamer and Lin, Jimmy},
	year = {2011},
	keywords = {anchor text, evaluation, hyperlinks, web},
	pages = {1073–1082},
	file = {ACM Full Text PDF:/Users/soboroff/Library/Application Support/Firefox/Profiles/6qwg5qr5.default/zotero/storage/73SDE24E/Asadi et al. - 2011 - Pseudo test collections for learning web search ra.pdf:application/pdf}
}